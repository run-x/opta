halt: false
environment_module: true
inputs:
  - name: env_name
    user_facing: false
    description: Opta Environment name
    default: None
  - name: layer_name
    user_facing: false
    description: Opta Layer name
    default: None
  - name: module_name
    user_facing: false
    description: Opta Module name
    default: None
  - name: labels
    user_facing: true
    validator: map(str(), str(), required=False)
    description: labels for the kubernetes nodes
    default: {}
  - name: max_nodes
    user_facing: true
    validator: any(str(), int(), required=False)
    description: Max number of nodes to allow via autoscaling
    default: 15
  - name: min_nodes
    user_facing: true
    validator: any(str(), int(), required=False)
    description: Min number of nodes to allow via autoscaling
    default: 3
  - name: node_disk_size
    user_facing: true
    validator: any(str(), int(), required=False)
    description: The size of disk to give the nodes' ec2s in GB.
    default: 20
  - name: node_instance_type
    user_facing: true
    validator: regex('^((?!(micro)|(nano)).)*$', required=False)
    description: The [ec2 instance type](https://aws.amazon.com/ec2/instance-types/) for the nodes.
    default: "t3.medium"
  - name: spot_instances
    user_facing: true
    validator: bool(required=False)
    description: |
      A boolean specifying whether to use [spot instances](https://aws.amazon.com/ec2/spot/)
      for the default nodegroup or not. The spot instances will be configured to have the max price equal to the on-demand
      price (so no danger of overcharging). _WARNING_: By using spot instances you must accept the real risk of frequent abrupt
      node terminations and possibly (although extremely rarely) even full blackouts (all nodes die). The former is a small
      risk as containers of Opta services will be automatically restarted on surviving nodes. So just make sure to specify
      a minimum of more than 1 containers -- Opta by default attempts to spread them out amongst many nodes. The former
      is a graver concern which can be addressed by having multiple node groups of different instance types (see aws
      nodegroup module) and ideally at least one non-spot.
    default: false
  - name: taints
    user_facing: true
    validator: list(include('taint'), required=False)
    description: Taints to add to the nodes in this nodegroup.
    default: []
  - name: use_gpu
    user_facing: true
    validator: bool(required=False)
    description: |
      Should we expect and use the gpus present in the ec2?
      Note: This input would be deprecated in the coming releases. Please switch to using `ami_type`.
      Usage: If using, `use_gpu: false`, just remove it. If using `use_gpu: true` replace it with `ami_type: AL2_x86_64_GPU`
    default: false
  - name: ami_type
    user_facing: true
    validator: enum("AL2_x86_64", "AL2_x86_64_GPU", "AL2_ARM_64", "BOTTLEROCKET_ARM_64", "BOTTLEROCKET_x86_64", required=False)
    description: |
      The AMI type to use for the nodes.
      For more information about this, please visit [here](https://docs.aws.amazon.com/eks/latest/APIReference/API_Nodegroup.html#AmazonEKS-Type-Nodegroup-amiType)
      Note: Currently, "CUSTOM" ami type is not supported.
    default: "AL2_x86_64"
outputs: {}
output_providers: {}
extra_validators:
  taint:
    key: str(required=True)
    value: str(required=False)
    effect: enum("NO_SCHEDULE", "NO_EXECUTE", "PREFER_NO_SCHEDULE", required=False)
output_data: {}
clouds:
  - aws